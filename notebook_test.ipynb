{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yaml         eval.py             run.py              \u001b[30m\u001b[43mvalidation\u001b[m\u001b[m\n",
      "dataset.py          model.py            tools.py\n",
      "\u001b[30m\u001b[43mdatasets\u001b[m\u001b[m            notebook_test.ipynb train.py\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 : TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description (see data analysis notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataset that we've created incorporates temporal features into the graph representations used in your graph neural network model. Here's a detailed description of the dataset considering the parameters `lag_step=4` and `lag_jump=10`:\n",
    "\n",
    "### Temporal Feature Construction:\n",
    "\n",
    "For each node in the graph at the current timestep `t`, you have constructed a feature vector that includes:\n",
    "\n",
    "1. The injection value at the current timestep `t`.\n",
    "2. The previous injection values at timesteps `t - lag_jump`, `t - 2*lag_jump`, `t - 3*lag_jump`, and `t - 4*lag_jump`. This captures the history of injections at intervals of 10 timesteps before the current timestep.\n",
    "3. Similarly, it includes the previous load values at timesteps `t - lag_jump`, `t - 2*lag_jump`, `t - 3*lag_jump`, and `t - 4*lag_jump`.\n",
    "\n",
    "### Parameters Description:\n",
    "\n",
    "- **lag_step**: This parameter defines how many previous timesteps you look back to construct the feature vector for each node. With `lag_step=4`, you are looking at four previous timesteps.\n",
    "- **lag_jump**: This parameter defines the interval between the considered timesteps. With `lag_jump=10`, you are considering every 10th timestep in the past. This means that for a current timestep `t`, you are using data from `t-10`, `t-20`, `t-30`, and `t-40` as part of the feature vector.\n",
    "\n",
    "### Dataset Structure:\n",
    "\n",
    "- The resulting feature vector for each node at each timestep is 9-dimensional. It consists of 4 past injections, 4 past loads, and the current injection value.\n",
    "- The initial timesteps where we cannot construct a full feature vector (because there aren't enough previous timesteps) are excluded from the dataset. This means the first few timesteps (specifically, the first 40 timesteps in this case) are not represented in your dataset.\n",
    "- For each graph in the dataset, the `edge_index` and `edge_attr` remain the same as in the original graph structure, representing the connections between nodes and their respective attributes (e.g., reactance).\n",
    "\n",
    "### Implications for the Model:\n",
    "\n",
    "- By including temporal features, your model can potentially learn patterns related to the evolution of loads and injections over time, which may help in predicting future loads more accurately.\n",
    "- The temporal resolution we've chosen (every 10 timesteps) suggests we're interested in capturing medium-term trends rather than very short-term fluctuations.\n",
    "- The model can now potentially identify and learn from the cyclic patterns, trends, and time-based dependencies present in the data, which could be crucial for tasks like load forecasting.\n",
    "\n",
    "This enriched dataset should provide your model with a more nuanced understanding of the grid's dynamics, allowing it to make more informed predictions that take into account not just the current state of the grid but also its recent history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated `GNNModel` includes suited layers to handle graph-structured data for tasks like load forecasting on an electricity grid. Here's a detailed breakdown of its features and functionalities:\n",
    "\n",
    "1. **Layer Normalization at Input (`ln1`)**:\n",
    "   - The input features `x` are first normalized using LayerNorm. This step can help stabilize learning by normalizing the features to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "2. **Convolutional Layers (`conv_layers`)**:\n",
    "   - The model employs Graph Attention Network Convolution (GATConv) or Graph Convolution Network (GCNConv) layers, chosen based on the `use_gat` flag.\n",
    "   - The first convolutional layer takes the input features and transforms them into `hidden_features`.\n",
    "\n",
    "3. **Residual Connections and Normalization**:\n",
    "   - From the second convolutional layer onwards, residual connections are used. The model adds the input of the layer (identity) to the output of the normalization layer.\n",
    "   - This approach is beneficial for deeper models, as it helps in mitigating the vanishing gradient problem and enables the model to learn more complex patterns.\n",
    "\n",
    "4. **Activation Function (`leaky_or_tanh_func`)**:\n",
    "   - The choice between Leaky ReLU and Tanh as the activation function is controlled by the `leaky_or_tanh` flag. This allows flexibility in model behavior and non-linearity.\n",
    "\n",
    "5. **Dropout for Regularization**:\n",
    "   - Dropout is applied after the activation function during training to prevent overfitting. It randomly zeroes some of the elements of the input tensor with probability `dropout` during training.\n",
    "\n",
    "6. **Layer Normalization for Edge Features (`ln2`)**:\n",
    "   - After computing the edge features (by concatenating the node features for each edge), these features are normalized using another LayerNorm layer (`ln2`).\n",
    "\n",
    "7. **Edge Features Transformation**:\n",
    "   - The model employs a linear transformation (`edge_transform`) followed by the final fully connected layer (`fc`) to generate the output features for each edge.\n",
    "\n",
    "8. **Attention Mechanism**:\n",
    "   - If `use_attention` is true, the model includes an `AttentionLayer` that applies an attention mechanism to the edge outputs. This can allow the model to focus on the most relevant edges, potentially improving the accuracy of predictions.\n",
    "\n",
    "9. **Output**:\n",
    "   - The final output of the model is the transformed edge output, which represents the model’s predictions.\n",
    "\n",
    "The architecture of this model is designed to process graph-structured data effectively by leveraging both node features and the structural information encoded in the graph's edges. The use of attention mechanisms and various normalization techniques aims to enhance the model's ability to capture and learn complex patterns within the data, which is crucial for accurate load forecasting in electrical grids. The flexibility in choosing convolution types (GAT or GCN) and activation functions allows for customization based on specific dataset characteristics and modeling requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test of the eval.py functionnality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.curdir\n",
    "\n",
    "config_path = cur_dir + '/config.yaml'\n",
    "dir_path = cur_dir + '/datasets'\n",
    "verbose = 2\n",
    "save_plot = \"\" # use --save_plot or --no-save_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the plot are generated and saved in the log folder of the model, we also include tensorboard visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The trained model and other data will be saved here : ./logs/20231221-230759\n",
      "\n",
      "device is set to cpu\n",
      "Training (epoch 1/25): 100%|█████████████████| 218/218 [00:01<00:00, 150.47it/s]\n",
      "Epoch 1, Avg Training Loss: 211.4559, MAE: 0.0448, RMSE: 0.0632, MAPE: 13792.6411, R-squared: 0.0003, Max Error: 0.2599\n",
      "Training (epoch 1/25): 100%|███████████████████| 94/94 [00:00<00:00, 187.75it/s]\n",
      "Epoch 1, Avg Validation Loss: 170.9469, MAE: 0.0802, RMSE: 0.1021, MAPE: 41666.1277, R-squared: 0.0010, Max Error: 0.3173\n",
      "Improved best validation loss at epoch 1 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 2/25): 100%|█████████████████| 218/218 [00:01<00:00, 169.16it/s]\n",
      "Epoch 2, Avg Training Loss: 176.3360, MAE: 0.0432, RMSE: 0.0588, MAPE: 20706.3326, R-squared: 0.0006, Max Error: 0.2365\n",
      "Training (epoch 2/25): 100%|███████████████████| 94/94 [00:00<00:00, 188.73it/s]\n",
      "Epoch 2, Avg Validation Loss: 159.9592, MAE: 0.0799, RMSE: 0.1042, MAPE: 44307.3245, R-squared: 0.0006, Max Error: 0.3072\n",
      "Improved best validation loss at epoch 2 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 3/25): 100%|█████████████████| 218/218 [00:01<00:00, 165.98it/s]\n",
      "Epoch 3, Avg Training Loss: 163.4489, MAE: 0.0429, RMSE: 0.0589, MAPE: 19259.3991, R-squared: 0.0009, Max Error: 0.2456\n",
      "Training (epoch 3/25): 100%|███████████████████| 94/94 [00:00<00:00, 161.40it/s]\n",
      "Epoch 3, Avg Validation Loss: 145.3472, MAE: 0.0799, RMSE: 0.1054, MAPE: 37093.7633, R-squared: 0.0004, Max Error: 0.3396\n",
      "Improved best validation loss at epoch 3 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 4/25): 100%|█████████████████| 218/218 [00:01<00:00, 158.22it/s]\n",
      "Epoch 4, Avg Training Loss: 146.9911, MAE: 0.0390, RMSE: 0.0591, MAPE: 17787.1984, R-squared: 0.0012, Max Error: 0.2370\n",
      "Training (epoch 4/25): 100%|███████████████████| 94/94 [00:00<00:00, 176.49it/s]\n",
      "Epoch 4, Avg Validation Loss: 127.7705, MAE: 0.0775, RMSE: 0.1029, MAPE: 32139.7473, R-squared: 0.0008, Max Error: 0.3392\n",
      "Improved best validation loss at epoch 4 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 5/25): 100%|█████████████████| 218/218 [00:01<00:00, 151.60it/s]\n",
      "Epoch 5, Avg Training Loss: 133.6058, MAE: 0.0345, RMSE: 0.0537, MAPE: 16768.1548, R-squared: 0.0017, Max Error: 0.2445\n",
      "Training (epoch 5/25): 100%|███████████████████| 94/94 [00:00<00:00, 163.28it/s]\n",
      "Epoch 5, Avg Validation Loss: 116.5816, MAE: 0.0745, RMSE: 0.0995, MAPE: 31489.6968, R-squared: 0.0015, Max Error: 0.3356\n",
      "Improved best validation loss at epoch 5 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 6/25): 100%|█████████████████| 218/218 [00:01<00:00, 166.39it/s]\n",
      "Epoch 6, Avg Training Loss: 124.3634, MAE: 0.0322, RMSE: 0.0494, MAPE: 13334.7431, R-squared: 0.0020, Max Error: 0.2534\n",
      "Training (epoch 6/25): 100%|███████████████████| 94/94 [00:00<00:00, 188.96it/s]\n",
      "Epoch 6, Avg Validation Loss: 109.1676, MAE: 0.0730, RMSE: 0.0989, MAPE: 35521.6676, R-squared: 0.0016, Max Error: 0.3274\n",
      "Improved best validation loss at epoch 6 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 7/25): 100%|█████████████████| 218/218 [00:01<00:00, 159.01it/s]\n",
      "Epoch 7, Avg Training Loss: 117.2815, MAE: 0.0308, RMSE: 0.0460, MAPE: 17742.9541, R-squared: 0.0021, Max Error: 0.2199\n",
      "Training (epoch 7/25): 100%|███████████████████| 94/94 [00:00<00:00, 151.95it/s]\n",
      "Epoch 7, Avg Validation Loss: 103.9213, MAE: 0.0720, RMSE: 0.0985, MAPE: 38874.2048, R-squared: 0.0017, Max Error: 0.3123\n",
      "Improved best validation loss at epoch 7 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 8/25): 100%|█████████████████| 218/218 [00:01<00:00, 175.60it/s]\n",
      "Epoch 8, Avg Training Loss: 112.2726, MAE: 0.0334, RMSE: 0.0510, MAPE: 14189.3624, R-squared: 0.0019, Max Error: 0.2144\n",
      "Training (epoch 8/25): 100%|███████████████████| 94/94 [00:00<00:00, 142.13it/s]\n",
      "Epoch 8, Avg Validation Loss: 100.0951, MAE: 0.0713, RMSE: 0.0979, MAPE: 37584.4388, R-squared: 0.0018, Max Error: 0.2984\n",
      "Improved best validation loss at epoch 8 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 9/25): 100%|█████████████████| 218/218 [00:01<00:00, 161.53it/s]\n",
      "Epoch 9, Avg Training Loss: 108.6590, MAE: 0.0299, RMSE: 0.0474, MAPE: 9737.9220, R-squared: 0.0021, Max Error: 0.2283\n",
      "Training (epoch 9/25): 100%|███████████████████| 94/94 [00:00<00:00, 175.95it/s]\n",
      "Epoch 9, Avg Validation Loss: 97.0667, MAE: 0.0704, RMSE: 0.0969, MAPE: 34310.2686, R-squared: 0.0020, Max Error: 0.2907\n",
      "Improved best validation loss at epoch 9 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 10/25): 100%|████████████████| 218/218 [00:01<00:00, 164.88it/s]\n",
      "Epoch 10, Avg Training Loss: 105.5934, MAE: 0.0292, RMSE: 0.0455, MAPE: 10397.0952, R-squared: 0.0023, Max Error: 0.2190\n",
      "Training (epoch 10/25): 100%|██████████████████| 94/94 [00:00<00:00, 178.27it/s]\n",
      "Epoch 10, Avg Validation Loss: 94.3908, MAE: 0.0697, RMSE: 0.0959, MAPE: 31775.0106, R-squared: 0.0021, Max Error: 0.2856\n",
      "Improved best validation loss at epoch 10 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 11/25): 100%|████████████████| 218/218 [00:01<00:00, 159.52it/s]\n",
      "Epoch 11, Avg Training Loss: 102.9419, MAE: 0.0342, RMSE: 0.0492, MAPE: 9276.6491, R-squared: 0.0019, Max Error: 0.2089\n",
      "Training (epoch 11/25): 100%|██████████████████| 94/94 [00:00<00:00, 189.35it/s]\n",
      "Epoch 11, Avg Validation Loss: 92.0016, MAE: 0.0696, RMSE: 0.0950, MAPE: 29790.8378, R-squared: 0.0023, Max Error: 0.2846\n",
      "Improved best validation loss at epoch 11 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 12/25): 100%|████████████████| 218/218 [00:01<00:00, 168.84it/s]\n",
      "Epoch 12, Avg Training Loss: 100.6911, MAE: 0.0344, RMSE: 0.0521, MAPE: 7204.5854, R-squared: 0.0020, Max Error: 0.2539\n",
      "Training (epoch 12/25): 100%|██████████████████| 94/94 [00:00<00:00, 187.45it/s]\n",
      "Epoch 12, Avg Validation Loss: 89.7181, MAE: 0.0694, RMSE: 0.0941, MAPE: 28512.3697, R-squared: 0.0024, Max Error: 0.2856\n",
      "Improved best validation loss at epoch 12 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 13/25): 100%|████████████████| 218/218 [00:01<00:00, 166.24it/s]\n",
      "Epoch 13, Avg Training Loss: 98.7381, MAE: 0.0293, RMSE: 0.0448, MAPE: 5591.9025, R-squared: 0.0024, Max Error: 0.2068\n",
      "Training (epoch 13/25): 100%|██████████████████| 94/94 [00:00<00:00, 141.68it/s]\n",
      "Epoch 13, Avg Validation Loss: 88.2143, MAE: 0.0694, RMSE: 0.0934, MAPE: 27075.6995, R-squared: 0.0026, Max Error: 0.2862\n",
      "Improved best validation loss at epoch 13 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 14/25): 100%|████████████████| 218/218 [00:01<00:00, 169.29it/s]\n",
      "Epoch 14, Avg Training Loss: 96.8304, MAE: 0.0308, RMSE: 0.0458, MAPE: 5903.0510, R-squared: 0.0024, Max Error: 0.2134\n",
      "Training (epoch 14/25): 100%|██████████████████| 94/94 [00:00<00:00, 153.42it/s]\n",
      "Epoch 14, Avg Validation Loss: 86.4696, MAE: 0.0692, RMSE: 0.0926, MAPE: 25854.4362, R-squared: 0.0027, Max Error: 0.2883\n",
      "Improved best validation loss at epoch 14 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 15/25): 100%|████████████████| 218/218 [00:01<00:00, 167.02it/s]\n",
      "Epoch 15, Avg Training Loss: 95.2312, MAE: 0.0284, RMSE: 0.0439, MAPE: 6196.8435, R-squared: 0.0025, Max Error: 0.1881\n",
      "Training (epoch 15/25): 100%|██████████████████| 94/94 [00:00<00:00, 181.81it/s]\n",
      "Epoch 15, Avg Validation Loss: 85.7393, MAE: 0.0687, RMSE: 0.0916, MAPE: 24852.7739, R-squared: 0.0029, Max Error: 0.2892\n",
      "Improved best validation loss at epoch 15 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 16/25): 100%|████████████████| 218/218 [00:01<00:00, 159.85it/s]\n",
      "Epoch 16, Avg Training Loss: 93.6258, MAE: 0.0283, RMSE: 0.0418, MAPE: 7861.7638, R-squared: 0.0026, Max Error: 0.2141\n",
      "Training (epoch 16/25): 100%|██████████████████| 94/94 [00:00<00:00, 182.19it/s]\n",
      "Epoch 16, Avg Validation Loss: 84.0554, MAE: 0.0673, RMSE: 0.0895, MAPE: 23769.5186, R-squared: 0.0032, Max Error: 0.2841\n",
      "Improved best validation loss at epoch 16 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 17/25): 100%|████████████████| 218/218 [00:01<00:00, 157.64it/s]\n",
      "Epoch 17, Avg Training Loss: 92.0361, MAE: 0.0292, RMSE: 0.0431, MAPE: 8208.1806, R-squared: 0.0023, Max Error: 0.2475\n",
      "Training (epoch 17/25): 100%|██████████████████| 94/94 [00:00<00:00, 178.91it/s]\n",
      "Epoch 17, Avg Validation Loss: 82.8998, MAE: 0.0661, RMSE: 0.0875, MAPE: 21814.0665, R-squared: 0.0036, Max Error: 0.2807\n",
      "Improved best validation loss at epoch 17 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 18/25): 100%|████████████████| 218/218 [00:01<00:00, 149.97it/s]\n",
      "Epoch 18, Avg Training Loss: 90.6181, MAE: 0.0278, RMSE: 0.0420, MAPE: 6011.9817, R-squared: 0.0026, Max Error: 0.1940\n",
      "Training (epoch 18/25): 100%|██████████████████| 94/94 [00:00<00:00, 186.76it/s]\n",
      "Epoch 18, Avg Validation Loss: 81.9030, MAE: 0.0657, RMSE: 0.0867, MAPE: 20161.1037, R-squared: 0.0037, Max Error: 0.2788\n",
      "Improved best validation loss at epoch 18 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 19/25): 100%|████████████████| 218/218 [00:01<00:00, 167.12it/s]\n",
      "Epoch 19, Avg Training Loss: 89.3838, MAE: 0.0299, RMSE: 0.0463, MAPE: 7003.8389, R-squared: 0.0023, Max Error: 0.1977\n",
      "Training (epoch 19/25): 100%|██████████████████| 94/94 [00:00<00:00, 154.57it/s]\n",
      "Epoch 19, Avg Validation Loss: 81.0000, MAE: 0.0658, RMSE: 0.0864, MAPE: 18724.2487, R-squared: 0.0037, Max Error: 0.2764\n",
      "Improved best validation loss at epoch 19 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 20/25): 100%|████████████████| 218/218 [00:01<00:00, 184.90it/s]\n",
      "Epoch 20, Avg Training Loss: 88.0586, MAE: 0.0282, RMSE: 0.0437, MAPE: 5285.6009, R-squared: 0.0024, Max Error: 0.1983\n",
      "Training (epoch 20/25): 100%|██████████████████| 94/94 [00:00<00:00, 155.86it/s]\n",
      "Epoch 20, Avg Validation Loss: 80.5323, MAE: 0.0663, RMSE: 0.0865, MAPE: 17554.1529, R-squared: 0.0037, Max Error: 0.2759\n",
      "Improved best validation loss at epoch 20 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 21/25): 100%|████████████████| 218/218 [00:01<00:00, 160.73it/s]\n",
      "Epoch 21, Avg Training Loss: 86.9115, MAE: 0.0293, RMSE: 0.0456, MAPE: 5775.0659, R-squared: 0.0024, Max Error: 0.2399\n",
      "Training (epoch 21/25): 100%|██████████████████| 94/94 [00:00<00:00, 186.27it/s]\n",
      "Epoch 21, Avg Validation Loss: 79.3594, MAE: 0.0657, RMSE: 0.0856, MAPE: 15577.3497, R-squared: 0.0039, Max Error: 0.2777\n",
      "Improved best validation loss at epoch 21 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 22/25): 100%|████████████████| 218/218 [00:01<00:00, 169.24it/s]\n",
      "Epoch 22, Avg Training Loss: 85.7045, MAE: 0.0268, RMSE: 0.0406, MAPE: 5718.2213, R-squared: 0.0027, Max Error: 0.1884\n",
      "Training (epoch 22/25): 100%|██████████████████| 94/94 [00:00<00:00, 188.67it/s]\n",
      "Epoch 22, Avg Validation Loss: 79.0187, MAE: 0.0647, RMSE: 0.0840, MAPE: 14393.9309, R-squared: 0.0041, Max Error: 0.2728\n",
      "Improved best validation loss at epoch 22 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 23/25): 100%|████████████████| 218/218 [00:01<00:00, 169.03it/s]\n",
      "Epoch 23, Avg Training Loss: 84.8299, MAE: 0.0263, RMSE: 0.0414, MAPE: 3261.5869, R-squared: 0.0029, Max Error: 0.2142\n",
      "Training (epoch 23/25): 100%|██████████████████| 94/94 [00:00<00:00, 188.23it/s]\n",
      "Epoch 23, Avg Validation Loss: 78.0007, MAE: 0.0632, RMSE: 0.0822, MAPE: 12902.9202, R-squared: 0.0044, Max Error: 0.2711\n",
      "Improved best validation loss at epoch 23 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 24/25): 100%|████████████████| 218/218 [00:01<00:00, 165.95it/s]\n",
      "Epoch 24, Avg Training Loss: 83.7850, MAE: 0.0269, RMSE: 0.0421, MAPE: 4171.2357, R-squared: 0.0027, Max Error: 0.2195\n",
      "Training (epoch 24/25): 100%|██████████████████| 94/94 [00:00<00:00, 174.35it/s]\n",
      "Epoch 24, Avg Validation Loss: 77.2258, MAE: 0.0615, RMSE: 0.0803, MAPE: 11588.8989, R-squared: 0.0047, Max Error: 0.2690\n",
      "Improved best validation loss at epoch 24 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n",
      "Training (epoch 25/25): 100%|████████████████| 218/218 [00:01<00:00, 169.20it/s]\n",
      "Epoch 25, Avg Training Loss: 82.9589, MAE: 0.0265, RMSE: 0.0406, MAPE: 5310.9375, R-squared: 0.0029, Max Error: 0.2100\n",
      "Training (epoch 25/25): 100%|██████████████████| 94/94 [00:00<00:00, 188.15it/s]\n",
      "Epoch 25, Avg Validation Loss: 76.6000, MAE: 0.0600, RMSE: 0.0785, MAPE: 10344.1416, R-squared: 0.0049, Max Error: 0.2664\n",
      "Improved best validation loss at epoch 25 : saving...\n",
      "\n",
      " ----------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "command = f\"python train.py {config_path} {dir_path} {verbose} {save_plot}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9bca1c7bba69285e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9bca1c7bba69285e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard  --logdir='/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m20231221-230759\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(copy the folder name and paste it below after 'logs/....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path_dataset = cur_dir + '/logs/20231221-230759' + '/config.yaml'\n",
    "dir_path_dataset = cur_dir + '/datasets'\n",
    "output_path_dataset = cur_dir + '/logs/20231221-230759'\n",
    "verbose_dataset = 2\n",
    "pred_only = \"\" # use --pred_only or --no-pred_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is set to cpu\n",
      "Evaluation (epoch 1/1): 100%|██████████████| 9960/9960 [00:13<00:00, 763.98it/s]\n",
      "\n",
      "Loads.npy file generated at : ./logs/20231221-230759/Preds/loads_20231221-231820.npy\n",
      "\n",
      "Epoch 1, Avg Evaluation Loss: 267.9728, MAE: 0.0006, RMSE: 0.0012, MAPE: 40.6160, R-squared: -0.0000, Max Error: 0.0038\n"
     ]
    }
   ],
   "source": [
    "command = f\"python eval.py {config_path_dataset} {dir_path_dataset} {output_path_dataset} {verbose_dataset} {pred_only}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: MODEL EVALUATION (underestimated prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "Training (epoch 25/25): 100%|████████████████| 218/218 [00:01<00:00, 169.20it/s]\n",
    "\n",
    "Epoch 25, Avg Training Loss: 82.9589, MAE: 0.0265, RMSE: 0.0406, MAPE: 5310.9375, R-squared: 0.0029, Max Error: 0.2100\n",
    "\n",
    "Training (epoch 25/25): 100%|██████████████████| 94/94 [00:00<00:00, 188.15it/s]\n",
    "\n",
    "Epoch 25, Avg Validation Loss: 76.6000, MAE: 0.0600, RMSE: 0.0785, MAPE: 10344.1416, R-squared: 0.0049, Max Error: 0.2664\n",
    "\n",
    "Improved best validation loss at epoch 25 : saving...\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance, as indicated by the provided metrics, suggests certain areas for improvement, especially considering the critical nature of underestimating loads in an electricity grid. Let's analyze the scores and discuss potential strategies to address the underestimation issue:\n",
    "\n",
    "Based on the updated metrics for your model's performance, here's an analysis focusing on addressing the underestimation issue in the context of electricity grid load forecasting:\n",
    "\n",
    "### Analysis of Model Performance:\n",
    "\n",
    "1. **Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)**:\n",
    "   - Both MAE and RMSE are relatively low, which initially seems positive. However, the specific values of these errors need to be contextualized against baseline models or domain-specific thresholds to fully understand their significance.\n",
    "\n",
    "2. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   - The MAPE values are quite high, especially in the validation phase. This suggests that the model's relative errors are significant, which is critical when considering the accuracy required in load forecasting.\n",
    "\n",
    "3. **R-squared**:\n",
    "   - The R-squared values are very low for both training and validation, indicating that the model is not capturing a substantial portion of the variance in the dataset.\n",
    "\n",
    "4. **Max Error**:\n",
    "   - The maximum errors observed are not negligible, particularly in the validation set. In the context of load forecasting, such errors could lead to significant issues, especially if they represent underestimations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Underestimation\n",
    "\n",
    "\n",
    "**Regular monitoring** of the model's performance in a real-world setting is crucial. Over time, as more data is collected and the behavior of the grid evolves, the model should be retrained or fine-tuned to adapt to these changes. \n",
    "\n",
    "Implementing these strategies requires a careful balance to avoid overfitting and to maintain the model's ability to generalize well to unseen data. **The goal is to reduce underestimation risks without significantly compromising overall accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Custom Loss Function for Asymmetric Penalties:\n",
    "\n",
    "We could introduce a loss function that penalizes underestimations more than overestimations. This approach specifically targets the critical nature of underestimation in load forecasting. We would need to design a custom loss function where errors below the actual values (underestimations) incur a higher penalty. For example, a weighted mean squared error where the weights are higher for underestimations. The model learns to err on the side of overestimation, reducing the risk associated with underestimating loads.\n",
    "\n",
    "#### 2. Enhanced Model Architecture and Feature Engineering:\n",
    "\n",
    "If not, we might try to improve the model’s ability to capture complex patterns in the data, potentially reducing underestimation errors. We will need to experiment with different neural network architectures, such as deeper networks or different types of layers (e.g., adding attention mechanisms). Additionally, explore feature engineering to include more relevant information or transform existing features for better representation. A more robust model that can better understand the nuances of the data, leading to more accurate load predictions.\n",
    "\n",
    "#### 3. Detailed Error Analysis and Model Monitoring:\n",
    "\n",
    "Finally, we will focus on understanding the specific scenarios where underestimations occur and continuously improve model performance. We will perform a thorough analysis of instances where the model underestimates, looking for patterns or common characteristics. Monitor the model's performance over time, especially during peak loads or unusual conditions. Regularly update the model with new data and insights. Identification of key factors leading to underestimation, allowing for targeted improvements. Adaptation of the model to changing conditions over time, maintaining its relevance and accuracy.\n",
    "\n",
    "**These strategies aim to directly tackle underestimation while maintaining the overall integrity and predictive power of the model. By focusing on asymmetric loss adjustment, enhancing the model's structure, and committing to ongoing analysis and updates, you can significantly mitigate the risks associated with load forecasting underestimation. Through this framework we could introduce new feature, improve the data quality, maybe try ensemble methods (specificly for the issue), add some post-processing adjustment (maybe compare ourself to physic-based algorithm) and all of this to develop a new training strategy.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m20231221-230759\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(copy the folder name and paste it below after 'logs/....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path_eval = cur_dir + '/logs/20231221-230759' + '/config.yaml'\n",
    "dir_path_eval = cur_dir + '/validation'\n",
    "output_path_eval = cur_dir + '/logs/20231221-230759'\n",
    "verbose_eval = 2\n",
    "pred_only = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is set to cpu\n",
      "Evaluation (epoch 1/1): 100%|██████████████████| 10/10 [00:00<00:00, 149.50it/s]\n",
      "\n",
      "Loads.npy file generated at : ./logs/20231221-230759/Preds/loads_20231221-232242.npy\n",
      "\n",
      "Epoch 1, Avg Evaluation Loss: 181.2689, MAE: 0.7335, RMSE: 1.3353, MAPE: 565.1479, R-squared: -0.0449, Max Error: 4.3679\n"
     ]
    }
   ],
   "source": [
    "command = f\"python eval.py {config_path_eval} {dir_path_eval} {output_path_eval} {verbose_eval} {pred_only}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results on a separate dataset provide valuable insights into the model's generalization capabilities and areas for improvement. Let's analyze these results:\n",
    "\n",
    "### Analysis of Model Performance on the Separate Dataset:\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: \n",
    "   - An MAE of 0.7335 indicates that, on average, the model's predictions are off by this amount. While the absolute figure might seem low, its significance is context-dependent and should be compared against domain-specific benchmarks or baseline models.\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**:\n",
    "   - The RMSE of 1.3353, being higher than the MAE, suggests the presence of some larger errors in the predictions. This could be indicative of the model struggling with certain instances in the data.\n",
    "\n",
    "3. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   - A very high MAPE of 565.1479% indicates that the model's predictions have significant relative errors. This is especially concerning in load forecasting, where accuracy is crucial.\n",
    "\n",
    "4. **R-squared (R²)**:\n",
    "   - A negative R² value of -0.0449 implies that the model performs worse than a simple mean-based model. This is a strong indicator that the model is not capturing the underlying trends and patterns in the data effectively.\n",
    "\n",
    "5. **Max Error**:\n",
    "   - A Max Error of 4.3679 is concerning, especially in load forecasting, where large errors can have substantial consequences.\n",
    "\n",
    "### Strategies for Improvement:\n",
    "\n",
    "Given these results, it's clear that the model requires significant improvements to be effective for practical use. Here are some strategies:\n",
    "\n",
    "1. **Model Complexity**\n",
    "\n",
    "2. **Feature Engineering and Selection**\n",
    "\n",
    "3. **Hyperparameter Tuning**\n",
    "\n",
    "4. **Training Procedure**\n",
    "\n",
    "5. **Data Quality and Preprocessing**\n",
    "\n",
    "6. **Error Analysis**\n",
    "\n",
    "7. **Model Monitoring and Updating**\n",
    "\n",
    "8. **Comparison with Baselines**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
